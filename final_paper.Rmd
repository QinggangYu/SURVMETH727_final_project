---
title: "Does collectivism modulate sentiment on COVID-19 pandemic?"
subtitle: "An examination of Twitter data in the United States"
author: "Tong Suo & Qinggang Yu"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: yes
    df_print: kable
    latex_engine: xelatex
bibliography: mybib.bib
csl: ASA.csl
link-citations: yes
mainfont: Times New Roman
sansfont: Times New Roman
fontsize: 12pt
documentclass: article
geometry: margin=1in
always_allow_html: yes
---

\pagebreak

```{r, include = FALSE}
library(knitr)
library(tidyverse)
library(foreign)
library(readxl)
library(stargazer)
library(ggrepel)
library(readr)
```

# Introduction

21st century has been by various and nonstop pandemics: acute respiratory syndrome (SARS) in 2003, H1N1 influenza in 2009, Ebola in 2015, and since the end of 2019 till today, COVID-19. Besides imposing enormous threats to people's physical health, pandemics also cast huge influence public sentiment. Since its outbreak, COVID-19 has brought grave consequences for people's mental health and mood, regardless of age or gender[@Philip2020; @Milne2020; @Phelps2020; @Zhang2020; @Terry2020; @Kumar2020; @Pfefferbaum2020]. There is no doubt that the ongoing pandemic has and will continue bringing disturbances to the public sentiment.

However, one cannot fully understand the sentiment pattern of any group or society without understanding its cultural underpinning. One important dimension that shapes a culture is where this culture fall along the spectrum of collectivism - individualism [@Markus1991]. ............ Evolutionary perspective [@Fincher2008] ........Social psychological perspective [@Eisenberger2007; @Salvador2020]

To understand the role of collectivism in shaping emotional experiences in the U.S. during COVID-19 pandemic, we focused on analyzing sentiments on one of the most popular social media, Twitter.


# Data

## Data collection

### Twitter data

The data of the present study were collected by live-streaming tweets during a three-week period from October 30th to November 19th. We targeted this period because it overlapped with the onset of the "third wave" of the COVID-19 pandemic in the U.S., when close to 2,700,000 new cases and more than 22,000 new deaths happened. All data collection was conducted within the U.S. by applying a bounding box defined by geographical coordinates of the U.S borders during tweet streaming. Further, we conducted two separate lines of collection. One collection streamed tweets containing words that are related to the COVID-19 pandemic, such as "covid", "corona", and "pandemic". The other collection streamed general tweets, which means we did not limit the collection to any specific keywords. However, politically-charged tweets were identified by using keywords such as "Trump", "Biden", "vote". Politically-charged tweets in the general tweet dataset were later dropped from analyses to avoid the confounding influence of election which happened during the time of data collection. Both keywords lists are shown below.

```{r keyword lists, eval = FALSE}
COVID_list <- c('covid', 'corona', 'pandemic', 'epidemic',
                'reopen', 'quarantine','social distance', 
                'cough', 'fever', 'mask', 'virus', 'infect',
                'contagious', 'stayhome')
Politics_list <- c('vote', "election", "reelection", "Trump",
                   "Donald Trump", "Biden", "Joe Biden",
                   "democrat", "republican", "ballot", "fraud",
                   "campaign", "liberal", "conservative", "senate",
                   "house", "electoral", "college", "candidate",
                   "rally", "turnout")
```

The COVID-related tweets were collected in Python using the tweepy package, which provides wrapper functions of the Twitter API. On each day, the collection starts from between 10am and 12pm and runs for 10 hours. Due to technical issues, the collection did not run properly for two days within the three-week period, so we collected an additional day of day on November 20th. A brief demonstration of the Python codes used for the data collection is as follows. The complete version of the codes is included in the supplementary materials.

```{python, eval = FALSE, python.reticulate = FALSE}
all_tweets = []
box = [-178.334698, 18.910360999999998, -66.949895, 49.384358]
#The listener, MyStreamListener, is customized to collect only COVID-related tweets
#and append the tweets to the container
myStream = tweepy.Stream(auth = api.auth, listener = MyStreamListener(36000), 
                         tweet_mode = 'extended')
myStream.filter(locations = box)
```

The general tweets were collected in R using the `rtweet` package. Since general tweets were collected without using keywords, we expected a bigger volume of data and decided to shorten the collection time to 6 hours per day. Putting different time zones into consideration, we divided the data collection into two 3-hour periods -- one in the morning (started randomly between 9am to 12pm EST), and one in the evening (started randomly between 6pm to 9pm EST). R codes for data collection on Day 2, Oct 31st is as follows. Full script for data collections across Oct 30th to Nov 19th, along with the codes for fixing broken tweets can be found in supplementary materials.

```{r general tweet streaming, eval = FALSE}
library(rtweet)

# Day 2 - 10/31/2020, am & pm
## start from 11:00 AM, EST 10/31/2020
stream_tweets(
  lookup_coords("usa"),
  timeout = 60*60*3,  #live streaming for 3hrs
  file_name = "COVID_III_20201031_am.json",
  language = "en",
  parse = FALSE
)

COVID_III_day2am  <- parse_stream("COVID_III_20201031_am.json")
saveRDS(COVID_III_day2am, "COVID_20201031_am.rds")

## start from 9:00 PM, EST 10/31/2020
stream_tweets(
  lookup_coords("usa"),
  timeout = 60*60*3,  #live streaming for 3hrs
  file_name = "COVID_III_20201031_pm.json",
  language = "en",
  parse = FALSE
)

COVID_III_day2pm  <- parse_stream("COVID_III_20201031_pm.json")
saveRDS(COVID_III_day2pm, "COVID_20201031_pm.rds")
```

All RDS files were read in, and we created `day` and `ampm` variables to identify when the tweets were collected before combining all datasets into one. For instance:

```{r day&ampm, eval = FALSE}
# day 2
## am
day2_am <- readRDS("COVID_20201031_am.rds")
day2_am$day <- 2
day2_am$ampm <- "am"
## pm
day2_pm <- readRDS("COVID_20201031_pm.rds")
day2_pm$day <- 2
day2_pm$ampm <- "pm"
```

To reduce the size of the dataframe, only relevant variables were kept. 

```{r combine, eval = FALSE}
library(tidyverse)
library(magrittr)

# combine all datasets into general.
general <- rbind( day1_pm,
                  # data collection started in the afternoon of day 1
                  day2_am, day2_pm,
                  day3_am, day3_pm,
                  day4_am, day4_pm,
                  day5_am, day5_pm,
                  day6_am, day6_pm,
                  day7_am, day7_pm,
                  day8_am, day8_pm,
                  day9_am, day9_pm,
                  day10_am, day10_pm,
                  day11_am, day11_pm,
                  day12_am, day12_pm,
                  day13_am, day13_pm,
                  day14_am, day14_pm,
                  day15_am, day15_pm,
                  day16_am, 
                  # day16_pm was missing due to improper data collection
                  day17_am, day17_pm,
                  day18_am, day18_pm,
                  day19_am, day19_pm,
                  day20_am, day20_pm,
                  day21_am, day21_pm)

# create variable ID to identify each tweet
general$ID <- c(1:nrow(general)) 

# identify data collected during 3rd wave of outbreak
general$wave <- 3

general <- general %>%
           select(., c('ID', "text", "is_quote", "is_retweet", 
                       "favorite_count", "retweet_count", 
                       "quote_count", "reply_count", "place_name",
                       "place_full_name", "place_type",
                       "country", "bbox_coords",
                       "day", "ampm", "wave"))
```

Finally, COVID-relevant tweets and politically-charged tweets were identified using the keywords presented above.

```{r general filtering, eval = FALSE}
general$covid <- grepl("covid|corona|pandemic|epidemic|reopen|quarantine|social distance|cough|fever|mask|virus|infect|contagious|stayhome", 
                       general$text, 
                       ignore.case = TRUE)

general$politics <- grepl("vote|election|reelection|Trump|Donald Trump|Biden|Joe Biden|democrat|republican|ballot|fraud|campaign|liberal|conservative|senate|house|electoral|college|candidate|rally|turnout", 
                          general$text, 
                          ignore.case = TRUE)
```

### State-level collectivism

The level of collectivism for each continental states of the U.S. (plus Hawaii) was retrieved from a previous study. The score ranges from 0 (least collectivistic) to 100 (most collectivistic). The collectivism score was calculated from eight demographic/social indicators of each state. Some sample indicators of collectivism include percentage of people living alone (reverse-coded), percentage of self-employed workers (reverse-coded), divorce to marriage ratio (reverse-coded), and percentage of households with grandchildren in them. Because the collectivism score for Hawaii is exceedingly high, due to a bunch of sociocultural reasons, we winsorized it to 3 standard deviations above the mean.

### Covariates

We included a few state-level covariates in our model. Some of them are demographic, such as population, median age, median household income, and the level of conservatism. These data were retrieved from the U.S. Census Bureau. We additionally include the number of new cases and the number of new deaths of COVID-19 for each state during our three-week data collection period, as the sentiment might covary with the progression of the pandemic. The COVID-19 data were retrieved from a public database maintained by the John Hopkins website.

## Data cleaning

We first mapped the collected tweets to the state they were originated. This was accomplished using the reverse-geocode package in Python for the covid-related tweets. The coordinate bounding box of each tweet, embedded in the tweet json object of each corresponding tweet, was first extracted and its midpoint was identified. The package then output the state where the midpoint coordinate falls in. Along this process, we also dropped tweets originating from outside the U.S. but were incidentially collected (because of the imperfect overlap between the bounding box and the U.S. territory), and tweets in other languages than English. A brief demonstration of the Python codes used for this process (for covid-related tweets) is as follows.

```{python, eval = FALSE, python.reticulate = FALSE}
def sort_data(dates):
    f_name = dates + '_covid.pkl'
    infile = open(f_name, "rb")
    curr_day = pickle.load(infile)
    infile.close()
    all_states = {}
    #coords is a panda dataframe containing the name and coordinates of each state
    for i in range(coords.shape[0]):
        place = coords.loc[i, "NAME"]
        all_states[place] = []
    for twt in curr_day:
        if hasattr(twt, "place") and hasattr(twt.place, "country_code") \
        and hasattr(twt.place, "bounding_box"):
            if (twt.place.country_code == "US") and (twt.lang == "en"):
                loc_box = twt.place.bounding_box.coordinates[0]
                loc = np.mean(loc_box, axis = 0)
                coordinates = (loc[1], loc[0])
                r = rg.search(coordinates)
                place = r[0]['admin1']
                all_states[place].append(twt)
    outname = 'bystate_' + f_name
    f = open(outname, "wb")
    pickle.dump(all_states, f)
    f.close()
    return all_states
    
dates = ['1030', '1031', '1102', '1103', '1105', '1106', '1107', '1108', 
          '1109', '1110', '1111', '1112', '1113', '1114', '1115', '1116',
          '1117', '1118', '1119', '1120']
for day in dates:
    all_states = sort_data(day)
    #further processing starts here
    #See code chunk under sentiment analysis
```

Corresponding geo-mapping procedure of the general tweets was conducting in R. The latitude and longitude information for each tweet was calculated using `lat_lng` function in `rtweet` package. We then referred to [this](https://stackoverflow.com/questions/8751497/latitude-longitude-coordinates-to-state-code-in-r) answer on Stack Overflow to map each pair of latitude and longitude to its corresponding state. U.S. geographical information maintained by [Database of Global Administrative Areas (GADM)](https://gadm.org/download_country_v3.html) was used to define the border of all states. Due to the slight misalignment between `lookup_coords("usa")` argument in `streat_tweets()` function and the boarder information maintained by GADM, some tweets received `NA` for its state geo-mapping and  were therefore considered to be sent outside of the U.S. These tweets were dropped from further analyses.

```{r general geo-mapping, eval = FALSE}
# get latitude and longitude for each tweet, 
# only keep tweets with latitude and longitude
general <- lat_lng(general, coords = c( "bbox_coords"))
general <- filter(general, !is.na(lat) & !is.na(lng))

# geo-mapping function
library(sf)
library(spData)

## pointsDF: A data.frame whose first column contains longitudes and
##           whose second column contains latitudes.
##
## states:   An sf MULTIPOLYGON object with 50 states plus DC.
##
## name_col: Name of a column in `states` that supplies the states'
##           names.

lonlat_to_state <- function(pointsDF,
                            states = spData::us_states,
                            name_col = "NAME") {
    ## Convert points data.frame to an sf POINTS object
    pts <- st_as_sf(pointsDF, coords = 1:2, crs = 4326)

    ## Transform spatial data to some planar coordinate system
    ## (e.g. Web Mercator) as required for geometric operations
    states <- st_transform(states, crs = 3857)
    pts <- st_transform(pts, crs = 3857)

    ## Find names of state (if any) intersected by each point
    state_names <- states[[name_col]]
    ii <- as.integer(st_intersects(pts, states))
    state_names[ii]
}

USA_gadm <- st_read(dsn = "gadm36_USA.gpkg", layer = "gadm36_USA_1")

# apply function to data frame
general <- general %>%
            mutate(., state = lonlat_to_state(select(general, c(lng, lat)), 
                      states = USA_gadm, name_col = "NAME_1"))

# drop tweets without state information
general <- filter(general, !is.na(state))
```

Due to the huge volume (around 2,000,000 observations) of the general tweets dataset, we created a sub-sample with the following criteria. We first examined the number of tweets each state had for each day. For states that had more than 200 tweets on a specific day, we randomly selected 200 tweets from that day; for states that had no more than 200 tweets on a specific day, we kept all tweets from that day. This sampling process returned us a total of 178160 observations which was comparable to COVID tweets dataset, and was easier for the system to handle. 

```{r general subsample, eval = FALSE}
general_summary <-  general %>% 
                    group_by(day, state) %>%
                    summarise(n = n()) %>%
                    mutate(small_sample = ifelse(n<=200, TRUE, FALSE)) %>%
                    select(day, state, small_sample)

general <- general %>%
           left_join(., general_summary, by = c("day", "state"))

general_sample <- bind_rows(
  general %>% 
    group_by(day, state) %>%
    filter(small_sample==TRUE),
  general %>% 
    group_by(day,state) %>%
    filter(small_sample==FALSE) %>%
    sample_n(200))
```

## Sentiment analysis

We performed sentiment analysis with two widely-used models: VADER and NRC Word-Emotion Association Lexicon. VADER stands for Valence Aware Dictionary and Sentiment Reasoner, and it is a lexicon and rule-based model that quantifies a given text in terms of its positivity or negativity. We adopted three scores output by VADER for each tweet: Compound score, which is the general sentiment that varies from -1 (most negative) to +1 (most positive); Positivity score, which is the ratio of positive words to the whole text, thus varies from 0 to 1; and Negativity score, which is the ratio of negative words to the whole text, thus varies from 0 to 1 as well.  
NRC Word-Emotion Association Lexicon (called NRC hereafter) is also a lexicon-based model that quantifies the emotion associated with a given text. The NRC include a list of words and the eight emotion categories they belong to: anger, fear, anticipation, trust, surprise, sadness, joy, and disgust (note that NRC also links words to the two sentiments, positive and negative, but we did not use them since those were already quantified in VADER). Therefore, for a given text, NRC outputs the number of words belonging to each emotion category. We adopted these eight scores of each tweet in the present study to examine the more specific emotions of the tweets as a funciton of collectivism.  
The python codes used to calculate the sentiment/emotion of the covid-related tweets are listed below:

```{python, eval = FALSE, python.reticulate = FALSE}
for day in dates:
    all_states = sort_data(day)
    #container for vader scores
    vader_state = {}
    for i in all_states.keys():
        vader_state[i] = {'pos':[], 'neg':[], 'neu':[], 'compound':[]}
    #container for NRC scores
    NRC_state = {}
    for i in all_states.keys():
        NRC_state[i] = {'fear': [], 'anger': [], 'anticipation': [], \
                       'trust': [], 'surprise': [], 'positive': [], \
                       'negative': [], 'sadness': [], 'disgust': [], 'joy': [], \
                       'length': [], 'sentence': []}
    #looping through the data                   
    for s in all_states.keys():
        curr_states = all_states[s]
        if len(curr_states) > 0:
            #fetch the text of each tweet
            for i in range(len(curr_states)):
                tweet = curr_states[i]
                if hasattr(tweet, "extended_tweet"):
                    text = tweet.extended_tweet["full_text"]
                else:
                    text = tweet.text
                #Vader analysis
                broken_sent = nltk.sent_tokenize(text)
                sent_scores = {'pos':[], 'neg':[], 'neu':[], 'compound':[]}
                for sentence in broken_sent:
                    vs = analyzer.polarity_scores(sentence)
                    for k in vs.keys():
                        sent_scores[k].append(vs[k])
                for k in sent_scores.keys():
                    text_score = sum(sent_scores[k])/len(sent_scores[k])
                    vader_state[s][k].append(text_score)
                #NRC analysis
                text_object = NRCLex(text)
                text_score = text_object.affect_count
                text_length = len(text_object.words)
                NRC_state[s]['length'].append(text_length)
                NRC_state[s]['sentence'].append(len(text_object.sentences))
                for k in text_score.keys():
                    NRC_state[s][k].append(text_score[k])
```

In correspondence with the sentiment analysis conducted on covid-related tweets, the equivalent processing was conducted on general tweets in R. First,  function `sentiment_loop` was defined to loop through all rows in the dataset and apply vader and NRC analysis on each piece of tweet text. 

```{r general sentiment function,eval= FALSE}
library(syuzhet)
library(vader)

i = 1
sentiment_loop <- function(df_list) {for (df in df_list){
  # apply vader analysis
  print(paste("vader analysis on set", i))
  df <- df %>% 
        mutate(., df$text %>% vader_df())
  
  #apply ncr analysis 
  print(paste("ncr analysis on set", i))
  df <- df %>%
    mutate(., df$text %>% 
           get_nrc_sentiment(., 
                             cl = NULL, 
                             language = "english"))
  
  #create & save df
  print(paste0("creating and saving chunk",i,"_sentiment.rds"))
  df <- select(df, c(ID, compound, pos, neu, neg, anger:positive))
  saveRDS(df, file = paste0('chunk',i,'_sentiment.rds'))
  rm(df)
  i = i + 1
}
  }
```

Then, to save execution time and memory, only the `ID` and `text` variables were extracted from the `general_sample` dataset and submitted to the function.

```{r general sentiment analysis, eval = FALSE}
general_sample_text <- general_sample %>%
                       ungroup() %>%
                       select(c(ID, text))

# split the resampled dataframe into resample_df_list
text_dflist <- (split(general_sample_text, (as.numeric(rownames(general_sample_text))-1) %/% 5000))

# call function
sentiment_loop(df_list =text_dflist)

# read in sentiment files
sample_sentiment <- list.files(pattern = "sentiment.rds") %>%
                      map_dfr(readRDS)
```

Finally, all sentiment analysis results were combined with the original dataset.

``` {r general sentiment combine, eval = FALSE}
# combine
general_sample <- general_sample %>% 
                  full_join(., sample_sentiment, by = c("ID"))

# Some other cleaning & wrangling
## drop unnecessary column
general_sample <- general_sample %>% 
                  select(., -c(small_sample))
## create variable topic
general_sample$topic <- "general"
```

Before conducting the main analysis on two datasets, we computed one more covariate, number of sentences in each tweet. The specific use of this variable will be explained in the statistical analysis for NRC in the following section. The same function was applied on both sets of data, and the following codes used general tweets dataset as an example.

```{r sentence count, eval = FALSE}
library(tokenizers)
library(qdapRegex)
library(sentimentr)

# remove urls
general_sample$text_cleaned <- rm_twitter_url(general_sample$text)

# initialize columns
## sentence_count_1: use get_sentences in sentimentr
general_sample$sentence_count_1 <- NA
## sentence_count_2: use count_sentences in tokenizers
word_count$sentence_count_2 <- NA 

n = 1
for (n in 1:nrow(general_sample)){
                # use get_sentences in sentimentr, after removing url
                general_sample$sentence_count_1[n] <-
                  get_sentences(general_sample$text_cleaned[n]) %>% 
                  unlist() %>%
                  length()
                
                # use count_sentences in tokenizers, after removing url
                general_sample$sentence_count_2[n] <-
                  count_sentences(general_sample$text_cleaned[n])

}

# check agreement, r = .93
cor(general_sample$sentence_count_1, general_sample$sentence_count_2)
```



## Statistial analysis

We used multiple regression analysis to predict the sentiment of each state using the state-level collectivism, after controlling for the relevant covariates. We first analyzed sentiment based on VADER. We aggregated the score (compound score, as well as the positivity and negativity score) of all the covid-related tweets of each state across the days of the data collection period. We repeated the same procedure for the general tweets. Hence, we obtained one compound score, one positivity score, and one negativity score for each state on covid-related tweets, and likewise on general tweets. We first fit a base model, using only collectivism to predict the sentiment scores. We next fit a model including all the covariates. All the predictors were z scored. Each regression was weighted by the number of tweets collected for each state, since greater number provides a more reliable estimate of the sentiment. The R codes for VADER analysis are included below:  

```{r read_RDS, echo = F, warning=F, message=F}
general_url <- "https://github.com/QinggangYu/SURVMETH727_final_project/raw/main/COVID_WaveIII_s9_stccount.rds"
download.file(general_url, "general_t.rds")
general_t <- readRDS('general_t.rds')
```

```{r VADER_processing, warning=F, message=F, results='asis'}
##COVID TWEETS
cov_url <- "https://raw.githubusercontent.com/QinggangYu/SURVMETH727_final_project/main/Predictors_US_states.csv"
cov <- read_csv(url(cov_url))
covid_t_url <- "https://raw.githubusercontent.com/QinggangYu/SURVMETH727_final_project/main/vader_sentiment.csv"
covid_t <- read_csv(url(covid_t_url))
cd_url <- "https://raw.githubusercontent.com/QinggangYu/SURVMETH727_final_project/main/covid_21day_covariates.csv"
case_death <- read_csv(url(cd_url))

covid_t_agg <- covid_t %>% 
  group_by(state) %>% 
  summarise_at(vars(compound:count), mean, na.rm = TRUE)

comb_data <- cov %>% 
  left_join(covid_t_agg, by = c("State" = "state"))

full_data <- comb_data %>% 
  left_join(case_death, by = c("State" = "state"))

full_data <- subset(full_data, !is.na(full_data$compound))

full_data[, c(3:23, 25:26)] <- lapply(full_data[, c(3:23, 25:26)], scale)
full_data[full_data$State == "Hawaii", "Collectivism"] <- 3


#COMPOUND SCORE

#base model(With weight)
b_model_w <- lm(compound ~ Collectivism, weights = count, data = full_data)

#with covariates (with weight)
cov_model_w <- lm(compound ~ Collectivism + Population + Income + Median_age + 
                    Conservatism + case_sum + death_sum, 
                  weights = count, data = full_data)

#Pos SCORE

#base model(With weight)
b_modelpos_w <- lm(pos ~ Collectivism, weights = count, data = full_data)

#with covariates (with weight)
cov_modelpos_w <- lm(pos ~ Collectivism + Population + Income + Median_age + 
                       Conservatism + case_sum + death_sum, 
                     weights = count, data = full_data)

#Neg SCORE

#base model(With weight)
b_modelneg_w <- lm(neg ~ Collectivism, weights = count, data = full_data)

#with covariates (with weight)
cov_modelneg_w <- lm(neg ~ Collectivism + Population + Income + Median_age + 
                       Conservatism + case_sum + death_sum,
                     weights = count, data = full_data)

##GENERAL TWEETS

general_t_agg <- general_t %>%
  #filter(politics == FALSE) %>%
  #filter(covid == TRUE) %>% 
  group_by(state) %>%
  summarise(
    compound = mean(vader_compound, na.rm = TRUE),
    pos = mean(vader_pos, na.rm = TRUE),
    neg = mean(vader_neg, na.rm = TRUE),
    count = n()
  )
    

comb_data_gen <- cov %>% 
  left_join(general_t_agg, by = c("State" = "state"))

full_data_gen <- comb_data_gen %>% 
  left_join(case_death, by = c("State" = "state"))

full_data_gen <- subset(full_data_gen, !is.na(full_data_gen$compound))

full_data_gen[, c(3:22, 24, 25)] <- lapply(full_data_gen[, c(3:22, 24, 25)], scale)

##COMPOUND SCORE

#base model(With weight)
b_gen_w <- lm(compound ~ Collectivism, weights = count, data = full_data_gen)

#with covariates (with weight)
cov_gen_w <- lm(compound ~ Collectivism + Population + Income + Median_age+ 
                  Conservatism + case_sum + death_sum, 
                  weights = count, data = full_data_gen)

##Pos SCORE

#base model(With weight)
b_genpos_w <- lm(pos ~ Collectivism, weights = count, data = full_data_gen)

#with covariates (with weight)
cov_genpos_w <- lm(pos ~ Collectivism + Population + Income + Median_age + 
                     Conservatism + case_sum + death_sum, 
                     weights = count, data = full_data_gen)

##Neg SCORE

#base model(With weight)
b_genneg_w <- lm(neg ~ Collectivism, weights = count, data = full_data_gen)

#with covariates (with weight)
cov_genneg_w <- lm(neg ~ Collectivism + Population + Income + Median_age + 
                     Conservatism + case_sum + death_sum, 
                     weights = count, data = full_data_gen)
```

We performed the same set of analyses for sentiment scores from NRC. Because there are eight specific emotions, and we did not have a priori predictions on which emotions were likely involved, we treated our analyses as exploratory. We fit the base model and the model with covariates on each emotion score, for both the covid-related tweets and general tweets. In addition to the covariates we included in VADER analysis, we also controlled for the averaged number of sentences in the NRC analysis. This is because NRC produced raw count of occurrence of the words belonging to each emotion category, thus longer text will have a higher number of the count. Because there are eight emotion categories, we corrected for multiple correction using the Bonferroni method, and set the threshold of significance to *p* < 0.00625 (0.5/8). The R codes for NRC analysis are included below:

```{r NRC_processing, warning=F, message=F}

NRC_url <- "https://raw.githubusercontent.com/QinggangYu/SURVMETH727_final_project/main/NRC_raw.csv"
covid_t <- read_csv(url(NRC_url))

covid_t_agg <- covid_t %>% 
  group_by(state) %>% 
  summarise_at(vars(fear:num_sentence), mean, na.rm = TRUE)

comb_data <- covid_t_agg %>% 
  left_join(case_death, by = "state")

full_data_NRC <- cov %>% 
  left_join(comb_data, by = c("State" = "state"))
full_data_NRC[, c(3:29, 31:34)] <- lapply(full_data_NRC[, c(3:29, 31:34)], scale)

#iterated fitting
varlist <- names(full_data_NRC)[c(20:24, 27:29)]
models <- lapply(varlist, function(x){
  lm(substitute(i ~ Collectivism + Population + Income + Median_age + Conservatism + 
                  num_sentence + case_sum + death_sum, list(i = as.name(x))), 
     weights = count, data = full_data_NRC)
})

general_t_agg <- general_t %>%
  #filter(politics == FALSE) %>% 
  group_by(state) %>%
  summarise_at(vars(anger:trust, sentence_count_1), mean, na.rm = TRUE)

general_t_count <- general_t %>%
  #filter(politics == FALSE) %>% 
  group_by(state) %>%
  summarise(count = n())

general_t_data <- left_join(general_t_agg, general_t_count, by = "state")
    

comb_data_gen <- general_t_data %>% 
  left_join(case_death, by = "state")

full_data_genNRC <- cov %>% 
  left_join(comb_data_gen, by = c("State" = "state"))

full_data_genNRC[, c(3:28, 30:31)] <- lapply(full_data_genNRC[, c(3:28, 30:31)], scale)

models_gen <- lapply(varlist, function(x){
  lm(substitute(i ~ Collectivism + Population + Income + Median_age + Conservatism + 
                  sentence_count_1 + case_sum + death_sum, 
                list(i = as.name(x))), weights = count, data = full_data_genNRC)
})
```

# Results

## VADER analysis

We found that when more collectivist states tweeted about COVID-19, the sentiment of the tweets tended to be more positive (Fig. 1). This was evident in both the base model (*b* = `r b_model_w$coefficients[2]`, *p* = `r summary(b_model_w)$coefficients[2,4]`), as well as after controlling for the covariates (*b* = `r cov_model_w$coefficients[2]`, *p* = `r summary(cov_model_w)$coefficients[2,4]`). The positivity of the sentiment likely arose from their tweets containing more positive words, although this was only statisitically significant in the base model (*b* = `r b_modelpos_w$coefficients[2]`, *p* = `r summary(b_modelpos_w)$coefficients[2,4]`). Collectivism did not have any effects on the negativity score of the covid-related tweets, either in the base model or after controlling the covariates , *p*s > .05. These effects are summarized in Table 1.

```{r Table1, echo=FALSE, warning=F, message=F, results='asis'}
stargazer(b_model_w, cov_model_w, b_modelpos_w, cov_modelpos_w, b_modelneg_w, cov_modelneg_w,
          title = "Vader Sentiment on Covid-related tweets", dep.var.labels = c("Compound Score", "Positivity Score", "Negativity Score"),
          covariate.labels = c("Collectivism", "Population", "Income", "Median age", "Conservatism", "case number", "death number"),
          column.sep.width = "1pt",
          omit.stat = c("ll", "aic", "bic", "f", "ser"))
```

```{r echo = F, warning=F, message=F}
plot_m <- lm(compound ~ Population + Income + Median_age + Conservatism + case_sum + death_sum, 
                  weights = count, data = full_data)

plot_m2 <- lm(Collectivism ~ Population + Income + Median_age + Conservatism + case_sum + death_sum, 
                  weights = count, data = full_data)

full_data$Compound_adjusted <- plot_m$residuals
full_data$Collectivism_adjusted <- plot_m2$residuals

ggplot(data = full_data, mapping = aes(x = Collectivism_adjusted, y = Compound_adjusted)) + 
  geom_point(aes(size = count)) + 
  geom_smooth(method = "lm") +
  geom_text_repel(aes(label = State),
                  label.padding = 0.25,
                  size = 2.5,
                  segment.size = 0.15,
                  segment.alpha = 1) +
  labs(x = "Adjusted Collectivism", y = "Adjusted Compound Score", size = "Number of tweets per day", 
       title = "Fig. 1. Collectivism predicts more positive sentiment in Covid-related tweets") +
  theme(legend.title = element_text(size = 8))
```

We observed a very different picture for the general tweets. In particular, the sentiment of the general tweets from more collectivist states are more negative (Fig. 2). This was evident in both the base model (*b* = `r b_gen_w$coefficients[2]`, *p* = `r summary(b_gen_w)$coefficients[2,4]`), as well as after controlling for the covariates (*b* = `r cov_gen_w$coefficients[2]`, *p* = `r summary(cov_gen_w)$coefficients[2,4]`). This negative sentiment was driven by tweets in collectivist states containing more negative words (*b* = `r cov_genneg_w$coefficients[2]`, *p* = `r summary(cov_genneg_w)$coefficients[2,4]`), as well as less positive words (*b* = `r cov_genpos_w$coefficients[2]`, *p* = `r summary(cov_genpos_w)$coefficients[2,4]`). This result was consistent before or after controlling the covariates (stats above depicting models including the covariates). These effects are summarized in Table 2.

```{r Table2, echo=FALSE, warning=F, message=F, results='asis'}
stargazer(b_gen_w, cov_gen_w, b_genpos_w, cov_genpos_w, b_genneg_w, cov_genneg_w,
          title = "Vader Sentiment on General tweets", dep.var.labels = c("Compound Score", "Positivity Score", "Negativity Score"),
          covariate.labels = c("Collectivism", "Population", "Income", "Median age", "Conservatism", "case number", "death number"),
          column.sep.width = "1pt",
          omit.stat = c("ll", "aic", "bic", "f", "ser"))
```

```{r echo = F, warning=F, message=F}
plot_m <- lm(compound ~ Population + Income + Median_age + Conservatism + case_sum + death_sum, 
                  weights = count, data = full_data_gen)

plot_m2 <- lm(Collectivism ~ Population + Income + Median_age + Conservatism + case_sum + death_sum, 
                  weights = count, data = full_data_gen)

full_data_gen$Compound_adjusted <- plot_m$residuals
full_data_gen$Collectivism_adjusted <- plot_m2$residuals

ggplot(data = full_data_gen, mapping = aes(x = Collectivism_adjusted, y = Compound_adjusted)) + 
  geom_point(aes(size = count)) + 
  geom_smooth(method = "lm") +
  geom_text_repel(aes(label = State),
                  label.padding = 0.25,
                  size = 2.5,
                  segment.size = 0.15,
                  segment.alpha = 1) +
  labs(x = "Adjusted Collectivism", y = "Adjusted Compound Score", size = "Number of tweets per day", 
       title = "Fig. 2. Collectivism predicts more negative sentiment in general tweets") +
  theme(legend.title = element_text(size = 8)) + 
  scale_size(range = c(0, 2))
```

## NRC analysis

The results for the effect of collectivism on specific emotions in covid-related tweets are illustrated in Fig. 3 (note that we plotted the 95% confidence interval of the estimate, hence the uncorrected estimate). Although there seems to be a few notable effects, only the effect on fear and trust survived the correction for multiple-comparison. Covid-related Tweets in more collectivist states tended to contain less sentiment of fear and trust.

```{r echo = F, warning=F, message=F}
coef_table <- data.frame(matrix(ncol = 3, nrow = 0))

for (i in 1:length(models)){
  m <- models[[i]]
  conf_int <- confint(m, "Collectivism")
  row <- c(m$coefficients[2], conf_int[1], conf_int[2])
  coef_table <- rbind(coef_table, row)
}
names(coef_table) = c("Coefficient", "lower", "upper")

coef_table$emotion <- varlist

ggplot(data = coef_table) +
  geom_linerange(aes(xmin = lower, xmax = upper, y = emotion, color = emotion)) + 
  geom_point(aes(x = Coefficient, y = emotion)) + 
  geom_vline(aes(xintercept = 0)) +
  labs(x = "Effect of Collectivism", y = "Emotion",
       title = "Fig. 3. Effect of collectivism on different emotions of covid-related tweets") +
  theme(legend.position = "none")
```

The results for the effect of collectivism on specific emotions in general tweets are illustrated in Fig. 3 (note that we plotted the 95% confidence interval of the estimate, hence the uncorrected estimate). Again, the effect of collectivism on a few emotions was notable in the uncorrected results. However, none of the effects survived the correction for multiple-comparison.

```{r echo = F, warning=F, message=F}
coef_table_gen <- data.frame(matrix(ncol = 3, nrow = 0))

for (i in 1:length(models_gen)){
  m <- models_gen[[i]]
  conf_int <- confint(m, "Collectivism")
  row <- c(m$coefficients[2], conf_int[1], conf_int[2])
  coef_table_gen <- rbind(coef_table_gen, row)
}
names(coef_table_gen) = c("Coefficient", "lower", "upper")

coef_table_gen$emotion <- varlist

ggplot(data = coef_table_gen) +
  geom_linerange(aes(xmin = lower, xmax = upper, y = emotion, color = emotion)) + 
  geom_point(aes(x = Coefficient, y = emotion)) + 
  geom_vline(aes(xintercept = 0)) +
  labs(x = "Effect of Collectivism", y = "Emotion",
       title = "Fig. 4. Effect of collectivism on different emotions of general tweets") +
  theme(legend.position = "none")
```


# Discussion

This study investigated how sentiment may change as a function of collectivism in the face of pathogen threat in the U.S. by analyzing sentiment in geo-mapped tweets within the U.S. during COVID-19 pandemic. We started with a pair of competing hypotheses regarding the role of collectivism in modulating people's emotion under pathogen threat, and our results suggested that both hypotheses may be true in some specific domain.

According to our results from vader analyses, in general, collectivism sensitizes people's reaction to pathogen threat, hereby leading to more negative sentiment. This is consistent with the evolutionary framework laid out by Fincher and his colleagues [-@Fincher2008]. However, when narrowed down to pandemic-specific topics, collectivism actually functioned as a buffer and predicted more positive sentiment. This is consistent with the social psychological perspective where the sense of social embedding and mutual interdependence can provide analgesic effect in the face of discomfort [@@Eisenberger2007; @Salvador2020]. Our NRC results showing that collectivism predicted the decrease of fear in covid-related tweet further supported this argument.

The rest of results from NRC analyses was less telling. In covid-related tweets, it was hard to interpret why collectivism predicted less trust  while predicting more positive sentiment at the same time. Moreover, collectivism did not predict any other specific positive sentiment. Similarly, in general tweets, collectivism failed to predict any specific negative sentiment. Putting these results together, it seems that one question awaiting for further investigations is that which specific positive and negative emotions drive people's general feeling in different domains in the time of pandemic. However, such results can also be interpreted as a limitation of the NRC analysis conducted in the present study, which we will return to later.

The present study took the advantage of web-based large-scale data to capture the moment-to-moment public sentiment during COVID-19. This method has several advantages over traditional lab-based studies. First, people's mood can be elusive and constantly changing, making self report a challenging task to subjects and an inaccurate measure to researchers. In contrast, tweets live-streamed over a long period of time can easily avoid both issues by capturing people's on-site expressions and sampling people from all over the country for multiple times. Secondly, while subjects in a lab-setting may be concerned of expressing inappropriate emotions or behaviors (e.g., emotion), people are feeling much more comfortable sharing their authentic feelings on social media such as Twitter. Thirdly, Twitter simply provided a bigger potential subject pool than most survey-based subject recruiting platforms. 

Finally, a few limitations must be acknowledged. First, though Twitter gave us access to a bigger group of subjects, this group cannot be considered as a representative sample of the U.S. or the world population. Twitter users tend to be more affluent and younger users who are likely to be the less affected people in the time of pandemic. Therefore, the sentiment we captured on Twitter may be an inaccurate approximate of the actual public sentiment in the U.S. in real life. Secondly, the data collection and processing processes were conducted in two software, R and Python. Though we tried our best to make all codes functionally equivalent across the two software, the small misalignment between R and Python may still have cast influence on the results of the study. Finally, NRC performed sentiment analyses on sentence level, and has been found to perform better on longer texts[@reagan2017sentiment]. However, most tweets in our sample were within 3 sentences. The limited length of tweets may have led the NRC analyses to be less accurate, which in term led to the less telling and intuitive results.

In spite of the above mentioned limitations, this study has shed new light on the various roles of collectivism in modulating sentiment under pathogen threat. Future studies may benefit from exploring the tole collectivism beyond the Western culture. Within the U.S., Hawaii state in the present study had clearly distinguished itself as a unique subculture from the mainstream American culture, and it is possible that collectivism functions in a different manner in Hawaiians. On a global scale, previous studies have found that social relations may not always bring the sense of warmth and comfort in East Asian cultures such as Japan [@Park2014]. Therefore, collectivism may also play a different role in these cultures.

# References

<div id="refs"></div>

# Supplementary materials
