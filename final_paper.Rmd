---
title: "Does collectivism modulate sentiment on COVID-19 pandemic?"
subtitle: "An examination of Twitter data in the United States"
author: "Tong Suo & Qinggang Yu"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: yes
    df_print: kable
    latex_engine: xelatex
bibliography: mybib.bib
csl: ASA.csl
link-citations: yes
mainfont: Times New Roman
sansfont: Times New Roman
fontsize: 12pt
documentclass: article
geometry: margin=1in
always_allow_html: yes
---

\pagebreak


```{r, include = FALSE}
library(knitr)
library(tidyverse)
library(foreign)
library(readxl)
library(stargazer)
library(ggrepel)
library(readr)
library(magrittr)
```

# Introduction

The 21st century may be an era of the infectious disease [@quammen2012spillover]. Indeed, the 21st century has been inflicted by various pandemics: SARS in 2003, H1N1 influenza in 2009, and since the end of 2019 till today, COVID-19. Besides imposing enormous threats to people's physical health, pandemics also cast huge influence on mental health. Since its outbreak, COVID-19 has brought grave consequences for people's emotion and psychological well-being, regardless of age or gender[@Philip2020; @Milne2020; @Phelps2020; @Zhang2020; @Terry2020; @Kumar2020; @Pfefferbaum2020]. Therefore, understanding how pathogen threat (like the current pandemic) shifts people's emotional patterns or sentiments presents as a timely investigation.   

However, one cannot fully understand the sentiment pattern of any group or society without understanding its cultural underpinning, given the profound way that culture shapes one's emotion [@tsai2006cultural; @tsai2007ideal]. The present study focused on one important dimension that cultures differ on --- collectivism-individualism [@Markus1991; @hofstede1984culture]. Whereas individualistic cultures emphasize freedom, personal choice, self-interest, personal achievements, and the agency and independence of the self, collectivist cultures emphasize social roles, obligations, social norms, and the importance of groups, relationships, and hierarchy.  

The construct of collectivism-individualism is particularly relevant to the discussion of pathogen threat because prior work has proposed that variation in historical pathogen prevalence across the globe is the very reason that cultures differ on this dimension [@Fincher2008]. Throughout the human history, individuals and social groups need to possess psychological mechanisms that serve the function of antipathogen defense in order to survive. One effective antipathogen defense mechanism is collectivism. First, collectivists make clear distinction between ingroup and outgroup, and are more likely to show ingroup favoritism and outgroup denigration [@house2004culture]. They are thus more cautious when being around of outgroup, who, in human history, are more likely to carry novel pathogens [@diamond1999guns]. Second, collectivist cultures have stronger social norms [@gelfand2006nature], including many that has direct consequences on antipathogen defense [@sherman1999darwinian]. Therefore, it stands to reason that people from collectivist cultures should be more wary of pathogen threat, and hence showing a more negative sentiment in response to pathogen threat.  

However, one may hypothesize the exact opposite. Collectivism comes with a greater range of social relationships. Social relationships, especially close ones, are typically considered as warm and soothing [@williams2008experiencing]. For instance, prior work show that physical reactions to pain are attentuated in the presence of close others [@coan2006lending; @Eisenberger2007]. Extrapolating from these findings, it is possible that when feeling embedded in social relationships, there may generate a so-called "analgesic effect", so as individuals become less sensitive and vigilant to the danger and threat in the environment. Indeed, a recent study shows that when primed with pathogen threat, individuals who are more interdependent (or collectivist) become less vigilant to norm violations [@Salvador2020]. Therefore, an alternative hypothesis is that people from collectivist cultures may in fact be less sensitive to pathogen threat, and thus exhibit a more positive (or less negative) sentiment towards the pandemic.

To test these competing hypotheses, we investigate people's expression of their sentiments towards the pandemic as they naturally occur. In particular, we prospectly collected posts (or tweets) from Twitter, one of the most popular social media, and analyzed the sentiments. This method enables us to collect a large amount of data which are not limited by geographical constraint. We focused our investigation on each of the states of the USA, which has been shown to differ in the degree of collectivism. We examined whether the tweets coming from different states differ in the sentiments, as a function of the level of collectivism of each state.


# Data

## Data collection

### Twitter data

The data of the present study were collected by live-streaming tweets during a three-week period from October 30th to November 19th. We targeted this period because it overlapped with the onset of the "third wave" of the COVID-19 pandemic in the U.S., when close to 2,700,000 new cases and more than 22,000 new deaths happened. All tweets were from within the U.S. as we applied a bounding box defined by geographical coordinates of the U.S borders during tweet streaming. We conducted two separate lines of collection. One collection streamed tweets containing words that are related to the COVID-19 pandemic, such as "covid", "corona", and "pandemic" (for the full list, see below). The other collection streamed general tweets, which means we did not limit the collection to any specific keywords. This general collection provides an estimate of the baseline sentiment pattern.

```{r covid list, eval = FALSE}
COVID_list <- c('covid', 'corona', 'pandemic', 'epidemic',
                'reopen', 'quarantine','social distance', 
                'cough', 'fever', 'mask', 'virus', 'infect',
                'contagious', 'stayhome')
```

The COVID-related tweets were collected in Python using the tweepy package, which provides wrapper functions of the Twitter API. On each day, the collection starts from between 10am and 12pm and runs for 10 hours. Due to technical issues, the collection did not run properly for two days within the three-week period, so we collected an additional day of data on November 20th. A brief demonstration of the Python codes used for the data collection is as follows. The complete version of the codes is included in the supplementary materials.

```{python, eval = FALSE, python.reticulate = FALSE}
all_tweets = []
box = [-178.334698, 18.910360999999998, -66.949895, 49.384358]
#The listener, MyStreamListener, is customized to collect only COVID-related tweets
#and append the tweets to the container
myStream = tweepy.Stream(auth = api.auth, listener = MyStreamListener(36000), 
                         tweet_mode = 'extended')
myStream.filter(locations = box)
```

The general tweets were collected in R using the `rtweet` package. Since general tweets were collected without using keywords, we expected a bigger volume of data and decided to shorten the collection time to 6 hours per day. We divided the data collection into two 3-hour periods -- one in the morning (started randomly between 9am to 12pm EST), and one in the evening (started randomly between 6pm to 9pm EST). These time periods were chosen to accommodate the three different time zones in the U.S. R codes for data collection on Day 2, Oct 31st is presented below as an example. Additional codes for fixing broken tweets can be found in supplementary materials.

```{r general tweet streaming, eval = FALSE}
library(rtweet)

# Day 2 - 10/31/2020, am & pm
## start from 11:00 AM, EST 10/31/2020
stream_tweets(
  lookup_coords("usa"),
  timeout = 60*60*3,  #live streaming for 3hrs
  file_name = "COVID_III_20201031_am.json",
  language = "en",
  parse = FALSE
)

COVID_III_day2am  <- parse_stream("COVID_III_20201031_am.json")
saveRDS(COVID_III_day2am, "COVID_20201031_am.rds")

## start from 9:00 PM, EST 10/31/2020
stream_tweets(
  lookup_coords("usa"),
  timeout = 60*60*3,  #live streaming for 3hrs
  file_name = "COVID_III_20201031_pm.json",
  language = "en",
  parse = FALSE
)

COVID_III_day2pm  <- parse_stream("COVID_III_20201031_pm.json")
saveRDS(COVID_III_day2pm, "COVID_20201031_pm.rds")
```

All RDS files were then read in. Time of data collection was properly labeled, and a unique ID was created for each tweet before collapsed into one dataframe. To reduce the size of the dataframe, only relevant variables were kept. 

Finally, because the data collection overlapped with the 2020 U.S. president election, another emotion-laden event that can greatly bias the sentiment, we labeled and removed tweets that were politically charged from the general tweets by using the following keywords.

```{r politics list, eval = FALSE}
Politics_list <- c('vote', "election", "reelection", "Trump",
"Donald Trump", "Biden", "Joe Biden",
"democrat", "republican", "ballot", "fraud",
"campaign", "liberal", "conservative", "senate",
"house", "electoral", "college", "candidate",
"rally", "turnout")
```

### State-level collectivism

The level of collectivism for each continental states of the U.S. (plus Hawaii) was retrieved from a previous study [@Vandello1999]. The score ranges from 0 (least collectivist) to 100 (most collectivist). The collectivism score was calculated from eight demographic/social indicators of each state. Sample indicators of collectivism include percentage of people living alone (reverse-coded), divorce to marriage ratio (reverse-coded), and percentage of households with grandchildren in them. Because the collectivism score for Hawaii is exceedingly high due to multiple sociocultural reasons [@triandis1990multimethod], we winsorized it to 3 standard deviations above the mean.

### Covariates

We included a few state-level covariates in our model. Some of them are demographic, such as population, median age, median household income, and the level of conservatism. These data were retrieved from the U.S. Census Bureau. We additionally included the number of new cases and the number of new deaths of COVID-19 for each state during our three-week data collection period, as the sentiment might covary with the progression of the pandemic. The COVID-19 data were retrieved from a public database maintained by the John Hopkins University.

## Data cleaning

We first mapped the collected tweets to the state they originated. This was accomplished using the reverse-geocode package in Python for the covid-related tweets. The coordinate bounding box of each tweet, embedded in the json object of each corresponding tweet, was first extracted, and its midpoint was identified. The function then output the state where the midpoint coordinate falls in. Along this process, we also dropped tweets originating from outside the U.S. but were incidentially collected (because of the imperfect overlap between the bounding box and the U.S. territory), and tweets in other languages than English. A brief demonstration of the Python codes used for this process (for covid-related tweets) is as follows. The full codes are included in supplementary materials

```{python, eval = FALSE, python.reticulate = FALSE}

#create container for tweets by state
all_states = {}
#coords is a panda dataframe containing the name (and coordinates) of each state
for i in range(coords.shape[0]):
    place = coords.loc[i, "NAME"]
    all_states[place] = []
#curr_day is the container for tweets on one day
for twt in curr_day:
    if hasattr(twt, "place") and hasattr(twt.place, "country_code") \
    and hasattr(twt.place, "bounding_box"):
        if (twt.place.country_code == "US") and (twt.lang == "en"):
            loc_box = twt.place.bounding_box.coordinates[0]
            loc = np.mean(loc_box, axis = 0)
            coordinates = (loc[1], loc[0])
            r = rg.search(coordinates)
            place = r[0]['admin1']
            all_states[place].append(twt)
```

Corresponding geo-mapping procedure for the general tweets was conducted in R. The latitude and longitude information for each tweet was calculated using `lat_lng` function in `rtweet` package. We then referred to [this](https://stackoverflow.com/questions/8751497/latitude-longitude-coordinates-to-state-code-in-r) answer on Stack Overflow to map each pair of latitude and longitude to its corresponding state. U.S. geographical information maintained by [Database of Global Administrative Areas (GADM)](https://gadm.org/download_country_v3.html) was used to define the border of all states. Due to the slight misalignment between `lookup_coords("usa")` argument in `streat_tweets()` function and the boarder information maintained by GADM, some tweets received `NA` for its state geo-mapping and  were therefore considered to be sent outside of the U.S. These tweets were dropped from further analyses.

```{r general geo-mapping, eval = FALSE}
# get latitude and longitude for each tweet, 
# only keep tweets with latitude and longitude
general <- lat_lng(general, coords = c( "bbox_coords"))
general <- filter(general, !is.na(lat) & !is.na(lng))

# geo-mapping function
library(sf)
library(spData)

## pointsDF: A data.frame whose first column contains longitudes and
##           whose second column contains latitudes.
##
## states:   An sf MULTIPOLYGON object with 50 states plus DC.
##
## name_col: Name of a column in `states` that supplies the states'
##           names.

lonlat_to_state <- function(pointsDF,
                            states = spData::us_states,
                            name_col = "NAME") {
    ## Convert points data.frame to an sf POINTS object
    pts <- st_as_sf(pointsDF, coords = 1:2, crs = 4326)

    ## Transform spatial data to some planar coordinate system
    ## (e.g. Web Mercator) as required for geometric operations
    states <- st_transform(states, crs = 3857)
    pts <- st_transform(pts, crs = 3857)

    ## Find names of state (if any) intersected by each point
    state_names <- states[[name_col]]
    ii <- as.integer(st_intersects(pts, states))
    state_names[ii]
}

USA_gadm <- st_read(dsn = "gadm36_USA.gpkg", layer = "gadm36_USA_1")

# apply function to data frame
general <- general %>%
            mutate(., state = lonlat_to_state(select(general, c(lng, lat)), 
                      states = USA_gadm, name_col = "NAME_1"))

# drop tweets without state information
general <- filter(general, !is.na(state))
```

Due to the huge volume (around 2,000,000 observations) of the general tweets dataset, we created a sub-sample with the following criteria. We first examined the number of tweets each state had for each day. For states that had more than 200 tweets on a specific day, we randomly selected 200 tweets from that day; for states that had no more than 200 tweets on a specific day, we kept all tweets from that day. This sampling process returned us a total of 178160 observations which was comparable to COVID tweets dataset, and was easier for the system to handle. 

```{r general subsample, eval = FALSE}
library(tidyverse)
library(magrittr)

general_summary <-  general %>% 
                    group_by(day, state) %>%
                    summarise(n = n()) %>%
                    mutate(small_sample = ifelse(n<=200, TRUE, FALSE)) %>%
                    select(day, state, small_sample)

general <- general %>%
           left_join(., general_summary, by = c("day", "state"))

general_sample <- bind_rows(
  general %>% 
    group_by(day, state) %>%
    filter(small_sample==TRUE),
  general %>% 
    group_by(day,state) %>%
    filter(small_sample==FALSE) %>%
    sample_n(200))
```

## Sentiment analysis

We performed sentiment analysis with two widely-used models: VADER and NRC Word-Emotion Association Lexicon. VADER stands for Valence Aware Dictionary and Sentiment Reasoner, and it is a lexicon and rule-based model that quantifies a given text in terms of its positivity or negativity. We adopted three scores output by VADER for each tweet: Compound score, which is the general sentiment that varies from -1 (most negative) to +1 (most positive); Positivity score, which is the ratio of positive words to the whole text, thus varies from 0 to 1; and Negativity score, which is the ratio of negative words to the whole text, thus varies from 0 to 1 as well.  
NRC Word-Emotion Association Lexicon (called NRC hereafter) is also a lexicon-based model that quantifies the emotion associated with a given text. The NRC include a list of words and the eight emotion categories they belong to: anger, fear, anticipation, trust, surprise, sadness, joy, and disgust (note that NRC also links words to the two sentiments, positive and negative, but we did not use them since those were already quantified in VADER). Therefore, for a given text, NRC outputs the number of words belonging to each emotion category. We adopted these eight scores of each tweet in the present study to examine the more specific emotions of the tweets as a funciton of collectivism.  
A brief demonstration of the python codes used to calculate the sentiment/emotion of the covid-related tweets are listed below. The full codes are included in the supplementary materials.

```{python, eval = FALSE, python.reticulate = FALSE}
    
##Vader analysis##
broken_sent = nltk.sent_tokenize(text) #text is the text of a tweet
sent_scores = {'pos':[], 'neg':[], 'neu':[], 'compound':[]}
for sentence in broken_sent:
    vs = analyzer.polarity_scores(sentence)
    for k in vs.keys():
        sent_scores[k].append(vs[k])
for k in sent_scores.keys():
    text_score = sum(sent_scores[k])/len(sent_scores[k])
    #vader_state is the container storing VADER score by state
    vader_state[s][k].append(text_score)

##NRC analysis##
text_object = NRCLex(text)
#I customized the NRCLex function to produce raw count of each emotion
text_score = text_object.affect_count
text_length = len(text_object.words)
#NRC_state is the container storing NRC score by state
NRC_state[s]['length'].append(text_length)
NRC_state[s]['sentence'].append(len(text_object.sentences))
for k in text_score.keys():
    NRC_state[s][k].append(text_score[k])
```

In correspondence with the sentiment analysis conducted on covid-related tweets, the equivalent processing was conducted on general tweets in R. First, function `sentiment_loop` was defined to loop through all rows in the dataset and apply vader and NRC analysis on each piece of tweet text. 

```{r general sentiment function,eval= FALSE}
library(syuzhet)
library(vader)

i = 1
sentiment_loop <- function(df_list) {for (df in df_list){
  # apply vader analysis
  print(paste("vader analysis on set", i))
  df <- df %>% 
        mutate(., df$text %>% vader_df())
  
  #apply ncr analysis 
  print(paste("ncr analysis on set", i))
  df <- df %>%
    mutate(., df$text %>% 
           get_nrc_sentiment(., 
                             cl = NULL, 
                             language = "english"))
  
  #create & save df
  print(paste0("creating and saving chunk",i,"_sentiment.rds"))
  df <- select(df, c(ID, compound, pos, neu, neg, anger:positive))
  saveRDS(df, file = paste0('chunk',i,'_sentiment.rds'))
  rm(df)
  i = i + 1
}
  }
```

Then, to save execution time and memory, only the `ID` and `text` variables were extracted from the `general_sample` dataset and submitted to the function. Finally, all sentiment analysis results were combined with the original dataset.

```{r general sentiment analysis, eval = FALSE}
general_sample_text <- general_sample %>%
                       ungroup() %>%
                       select(c(ID, text))

# split the resampled dataframe into resample_df_list
text_dflist <- (split(general_sample_text, (as.numeric(rownames(general_sample_text))-1) %/% 5000))

# call function
sentiment_loop(df_list =text_dflist)

# read in sentiment files
sample_sentiment <- list.files(pattern = "sentiment.rds") %>%
                      map_dfr(readRDS)
```


``` {r general sentiment combine, include = FALSE, eval = FALSE}
# combine
general_sample <- general_sample %>% 
                  full_join(., sample_sentiment, by = c("ID"))

# Some other cleaning & wrangling
## drop unnecessary column
general_sample <- general_sample %>% 
                  select(., -c(small_sample))
## create variable topic
general_sample$topic <- "general"
```

Before conducting the main analysis on two datasets, we computed one more covariate, number of sentences in each tweet. The specific use of this variable will be explained in the statistical analysis for NRC in the following section. The same function was applied on both sets of data, and the following codes used general tweets dataset as an example.

```{r sentence count, eval = FALSE}
library(tokenizers)
library(qdapRegex)

# remove urls
general_sample$text_cleaned <- rm_twitter_url(general_sample$text)

# initialize columns
## sentence_count_1: use get_sentences in sentimentr
general_sample$sentence_count_1 <- NA

n = 1
for (n in 1:nrow(general_sample)){
                # use get_sentences in sentimentr, after removing url
                general_sample$sentence_count_1[n] <-
                  get_sentences(general_sample$text_cleaned[n]) %>% 
                  unlist() %>%
                  length()
}
```

```{r sentence count 2, include = FALSE, eval = FALSE}
# We computed sentence count using two packages, tokenizers and sentimentr.
# Here are the complete codes.
# We only used the sentence count computed by sentimentr in the analyses, so the code for computing sentence count using sentimentr is not shown in the main text.
# The results given by the two packages reach high agreement, r = .93.
# Here is the complete set of codes.

{r sentence count, eval = FALSE}
library(tokenizers)
library(qdapRegex)
library(sentimentr)

# remove urls
general_sample$text_cleaned <- rm_twitter_url(general_sample$text)

# initialize columns
## sentence_count_1: use get_sentences in sentimentr
general_sample$sentence_count_1 <- NA
## sentence_count_2: use count_sentences in tokenizers
word_count$sentence_count_2 <- NA 

n = 1
for (n in 1:nrow(general_sample)){
                # use get_sentences in sentimentr, after removing url
                general_sample$sentence_count_1[n] <-
                  get_sentences(general_sample$text_cleaned[n]) %>% 
                  unlist() %>%
                  length()
                
                # use count_sentences in tokenizers, after removing url
                general_sample$sentence_count_2[n] <-
                  count_sentences(general_sample$text_cleaned[n])

}

# check agreement, r = .93
cor(general_sample$sentence_count_1, general_sample$sentence_count_2)
```


## Statistial analysis

We used multiple regression analysis to predict the sentiment of each state using the state-level collectivism. We first analyzed sentiment based on VADER. We aggregated the score (compound score, as well as the positivity and negativity score) of all the covid-related tweets of each state across the days of the data collection period. We repeated the same procedure for the general tweets. Hence, we obtained one compound score, one positivity score, and one negativity score for each state on covid-related tweets, and likewise on general tweets. We first fit a base model, using only collectivism to predict the sentiment scores. We next fit a model including all the covariates. All the predictors were z scored. Each regression was weighted by the number of tweets collected for each state, since greater number provides a more reliable estimate of the sentiment. The R codes for VADER analysis are included below:  

```{r read_RDS, echo = F, warning=F, message=F}
general_url <- "https://github.com/QinggangYu/SURVMETH727_final_project/raw/main/COVID_WaveIII_s9_stccount.rds"
download.file(general_url, "general_t.rds")
general_t <- readRDS('general_t.rds')
```

```{r VADER_processing, warning=F, message=F, results='asis'}
##COVID TWEETS
cov_url <- "https://raw.githubusercontent.com/QinggangYu/SURVMETH727_final_project/main/Predictors_US_states.csv"
cov <- read_csv(url(cov_url))
covid_t_url <- "https://raw.githubusercontent.com/QinggangYu/SURVMETH727_final_project/main/vader_sentiment.csv"
covid_t <- read_csv(url(covid_t_url))
cd_url <- "https://raw.githubusercontent.com/QinggangYu/SURVMETH727_final_project/main/covid_21day_covariates.csv"
case_death <- read_csv(url(cd_url))

covid_t_agg <- covid_t %>% 
  group_by(state) %>% 
  summarise_at(vars(compound:count), mean, na.rm = TRUE)

comb_data <- cov %>% 
  left_join(covid_t_agg, by = c("State" = "state"))

full_data <- comb_data %>% 
  left_join(case_death, by = c("State" = "state"))

full_data <- subset(full_data, !is.na(full_data$compound))

full_data[, c(3:23, 25:26)] <- lapply(full_data[, c(3:23, 25:26)], scale)
full_data[full_data$State == "Hawaii", "Collectivism"] <- 3


#COMPOUND SCORE

#base model(With weight)
b_model_w <- lm(compound ~ Collectivism, weights = count, data = full_data)
#with covariates (with weight)
cov_model_w <- lm(compound ~ Collectivism + Population + Income + Median_age + 
                    Conservatism + case_sum + death_sum, 
                  weights = count, data = full_data)

#Pos SCORE

#base model(With weight)
b_modelpos_w <- lm(pos ~ Collectivism, weights = count, data = full_data)
#with covariates (with weight)
cov_modelpos_w <- lm(pos ~ Collectivism + Population + Income + Median_age + 
                       Conservatism + case_sum + death_sum, 
                     weights = count, data = full_data)

#Neg SCORE

#base model(With weight)
b_modelneg_w <- lm(neg ~ Collectivism, weights = count, data = full_data)
#with covariates (with weight)
cov_modelneg_w <- lm(neg ~ Collectivism + Population + Income + Median_age + 
                       Conservatism + case_sum + death_sum,
                     weights = count, data = full_data)


##GENERAL TWEETS (the chuck for reading the general tweet dataset from github
#                 was not shown in pdf, because echo = T does not work)

general_t_agg <- general_t %>%
  #filter(politics == FALSE) %>%
  #filter(covid == TRUE) %>% 
  group_by(state) %>%
  summarise(
    compound = mean(vader_compound, na.rm = TRUE),
    pos = mean(vader_pos, na.rm = TRUE),
    neg = mean(vader_neg, na.rm = TRUE),
    count = n()
  )
    

comb_data_gen <- cov %>% 
  left_join(general_t_agg, by = c("State" = "state"))

full_data_gen <- comb_data_gen %>% 
  left_join(case_death, by = c("State" = "state"))

full_data_gen <- subset(full_data_gen, !is.na(full_data_gen$compound))
full_data_gen[, c(3:22, 24, 25)] <- lapply(full_data_gen[, c(3:22, 24, 25)], scale)

##COMPOUND SCORE

#base model(With weight)
b_gen_w <- lm(compound ~ Collectivism, weights = count, data = full_data_gen)
#with covariates (with weight)
cov_gen_w <- lm(compound ~ Collectivism + Population + Income + Median_age+ 
                  Conservatism + case_sum + death_sum, 
                  weights = count, data = full_data_gen)

##Pos SCORE

#base model(With weight)
b_genpos_w <- lm(pos ~ Collectivism, weights = count, data = full_data_gen)
#with covariates (with weight)
cov_genpos_w <- lm(pos ~ Collectivism + Population + Income + Median_age + 
                     Conservatism + case_sum + death_sum, 
                     weights = count, data = full_data_gen)

##Neg SCORE

#base model(With weight)
b_genneg_w <- lm(neg ~ Collectivism, weights = count, data = full_data_gen)
#with covariates (with weight)
cov_genneg_w <- lm(neg ~ Collectivism + Population + Income + Median_age + 
                     Conservatism + case_sum + death_sum, 
                     weights = count, data = full_data_gen)
```

We performed the same set of analyses for sentiment scores from NRC. Because there are eight specific emotions, and we did not have a priori predictions on which emotions were likely involved, we treated our analyses as exploratory. We fit the base model and the model with covariates on each emotion score, for both the covid-related tweets and general tweets. In addition to the covariates we included in VADER analysis, we also controlled for the averaged number of sentences in the NRC analysis. This is because NRC produced raw count of occurrence of the words belonging to each emotion category, thus longer text will have a higher number of the count. Because there are eight emotion categories, we corrected for multiple correction using the Bonferroni method, and set the threshold of significance to *p* < 0.00625 (0.5/8). The R codes for NRC analysis are included below:

```{r NRC_processing, warning=F, message=F}

#COVID Tweet
NRC_url <- "https://raw.githubusercontent.com/QinggangYu/SURVMETH727_final_project/main/NRC_raw.csv"
covid_t <- read_csv(url(NRC_url))

covid_t_agg <- covid_t %>% 
  group_by(state) %>% 
  summarise_at(vars(fear:num_sentence), mean, na.rm = TRUE)

comb_data <- covid_t_agg %>% 
  left_join(case_death, by = "state")

full_data_NRC <- cov %>% 
  left_join(comb_data, by = c("State" = "state"))
full_data_NRC[, c(3:29, 31:34)] <- lapply(full_data_NRC[, c(3:29, 31:34)], scale)

#iterated fitting
varlist <- names(full_data_NRC)[c(20:24, 27:29)]
models <- lapply(varlist, function(x){
  lm(substitute(i ~ Collectivism + Population + Income + Median_age + Conservatism + 
                  num_sentence + case_sum + death_sum, list(i = as.name(x))), 
     weights = count, data = full_data_NRC)
})


#General Tweet
general_t_agg <- general_t %>%
  #filter(politics == FALSE) %>% 
  group_by(state) %>%
  summarise_at(vars(anger:trust, sentence_count_1), mean, na.rm = TRUE)

general_t_count <- general_t %>%
  #filter(politics == FALSE) %>% 
  group_by(state) %>%
  summarise(count = n())

general_t_data <- left_join(general_t_agg, general_t_count, by = "state")
    

comb_data_gen <- general_t_data %>% 
  left_join(case_death, by = "state")

full_data_genNRC <- cov %>% 
  left_join(comb_data_gen, by = c("State" = "state"))

full_data_genNRC[, c(3:28, 30:31)] <- lapply(full_data_genNRC[, c(3:28, 30:31)], scale)

models_gen <- lapply(varlist, function(x){
  lm(substitute(i ~ Collectivism + Population + Income + Median_age + Conservatism + 
                  sentence_count_1 + case_sum + death_sum, 
                list(i = as.name(x))), weights = count, data = full_data_genNRC)
})
```

# Results

## VADER analysis

We found that when more collectivist states tweeted about COVID-19, the sentiment of the tweets tended to be more positive (Fig. 1). This was evident in both the base model (*b* = `r b_model_w$coefficients[2] %>% round(digits = 3)`, *p* = `r summary(b_model_w)$coefficients[2,4] %>% round(digits = 3)`), as well as after controlling for the covariates (*b* = `r cov_model_w$coefficients[2] %>% round(digits = 3)`, *p* = `r summary(cov_model_w)$coefficients[2,4] %>% round(digits = 3)`). The positivity of the sentiment likely arose from their tweets containing more positive words, although this was only statisitically significant in the base model (*b* = `r b_modelpos_w$coefficients[2] %>% round(digits = 3)` , *p* = `r summary(b_modelpos_w)$coefficients[2,4] %>% round(digits = 3)`). Collectivism did not have any effects on the negativity score of the covid-related tweets, either in the base model or after controlling the covariates , *p*s > .05. These effects are summarized in Table 1.

```{r Table1, echo=FALSE, warning=F, message=F, results='asis'}
stargazer(b_model_w, cov_model_w, b_modelpos_w, cov_modelpos_w, b_modelneg_w, cov_modelneg_w,
          title = "Vader Sentiment on Covid-related tweets", dep.var.labels = c("Compound Score", "Positivity Score", "Negativity Score"),
          covariate.labels = c("Collectivism", "Population", "Income", "Median age", "Conservatism", "case number", "death number"),
          column.sep.width = "1pt",
          omit.stat = c("ll", "aic", "bic", "f", "ser"),
          header = FALSE)
```

```{r echo = F, warning=F, message=F}
plot_m <- lm(compound ~ Population + Income + Median_age + Conservatism + case_sum + death_sum, 
                  weights = count, data = full_data)

plot_m2 <- lm(Collectivism ~ Population + Income + Median_age + Conservatism + case_sum + death_sum, 
                  weights = count, data = full_data)

full_data$Compound_adjusted <- plot_m$residuals
full_data$Collectivism_adjusted <- plot_m2$residuals

ggplot(data = full_data, mapping = aes(x = Collectivism_adjusted, y = Compound_adjusted)) + 
  geom_point(aes(size = count)) + 
  geom_smooth(method = "lm") +
  geom_text_repel(aes(label = State),
                  label.padding = 0.25,
                  size = 2.5,
                  segment.size = 0.15,
                  segment.alpha = 1) +
  labs(x = "Adjusted Collectivism", y = "Adjusted Compound Score", size = "Number of tweets per day", 
       title = "Fig. 1. Collectivism predicts more positive sentiment in Covid-related tweets") +
  theme(legend.title = element_text(size = 8))
```

We observed a very different picture for the general tweets. In particular, the sentiment of the general tweets from more collectivist states are more negative (Fig. 2). This was evident in both the base model (*b* = `r b_gen_w$coefficients[2] %>% round(digits = 3)`, *p* < .001), as well as after controlling for the covariates (*b* = `r cov_gen_w$coefficients[2] %>% round(digits = 3)`, *p* < .001). This negative sentiment was driven by tweets in collectivist states containing more negative words (*b* = `r cov_genneg_w$coefficients[2] %>% round(digits = 3)`, *p* < .001), as well as less positive words (*b* = `r cov_genpos_w$coefficients[2] %>% round(digits = 3)`, *p* = `r summary(cov_genpos_w)$coefficients[2,4] %>% round(digits = 3)`). This result was consistent before or after controlling the covariates (stats above depicting models including the covariates). These effects are summarized in Table 2.

```{r Table2, echo=FALSE, warning=F, message=F, results='asis'}
stargazer(b_gen_w, cov_gen_w, b_genpos_w, cov_genpos_w, b_genneg_w, cov_genneg_w,
          title = "Vader Sentiment on General tweets", dep.var.labels = c("Compound Score", "Positivity Score", "Negativity Score"),
          covariate.labels = c("Collectivism", "Population", "Income", "Median age", "Conservatism", "case number", "death number"),
          column.sep.width = "1pt",
          omit.stat = c("ll", "aic", "bic", "f", "ser"),
          header = FALSE)
```

```{r echo = F, warning=F, message=F}
plot_m <- lm(compound ~ Population + Income + Median_age + Conservatism + case_sum + death_sum, 
                  weights = count, data = full_data_gen)

plot_m2 <- lm(Collectivism ~ Population + Income + Median_age + Conservatism + case_sum + death_sum, 
                  weights = count, data = full_data_gen)

full_data_gen$Compound_adjusted <- plot_m$residuals
full_data_gen$Collectivism_adjusted <- plot_m2$residuals

ggplot(data = full_data_gen, mapping = aes(x = Collectivism_adjusted, y = Compound_adjusted)) + 
  geom_point(aes(size = count)) + 
  geom_smooth(method = "lm") +
  geom_text_repel(aes(label = State),
                  label.padding = 0.25,
                  size = 2.5,
                  segment.size = 0.15,
                  segment.alpha = 1) +
  labs(x = "Adjusted Collectivism", y = "Adjusted Compound Score", size = "Number of tweets per day", 
       title = "Fig. 2. Collectivism predicts more negative sentiment in general tweets") +
  theme(legend.title = element_text(size = 8)) + 
  scale_size(range = c(0, 2))
```

## NRC analysis

The results for the effect of collectivism on specific emotions in covid-related tweets are illustrated in Fig. 3 (note that we plotted the 95% confidence interval of the estimate, hence the uncorrected estimate). Although there seems to be a few notable effects, only the effect on fear and trust survived the correction for multiple-comparison. Covid-related tweets in more collectivist states tended to contain less sentiment of fear and trust.

```{r echo = F, warning=F, message=F}
coef_table <- data.frame(matrix(ncol = 3, nrow = 0))

for (i in 1:length(models)){
  m <- models[[i]]
  conf_int <- confint(m, "Collectivism")
  row <- c(m$coefficients[2], conf_int[1], conf_int[2])
  coef_table <- rbind(coef_table, row)
}
names(coef_table) = c("Coefficient", "lower", "upper")

coef_table$emotion <- varlist

ggplot(data = coef_table) +
  geom_linerange(aes(xmin = lower, xmax = upper, y = emotion, color = emotion)) + 
  geom_point(aes(x = Coefficient, y = emotion)) + 
  geom_vline(aes(xintercept = 0)) +
  labs(x = "Effect of Collectivism", y = "Emotion",
       title = "Fig. 3. Effect of collectivism on different emotions of covid-related tweets") +
  theme(legend.position = "none")
```

The results for the effect of collectivism on specific emotions in general tweets are illustrated in Fig. 4 (note that we plotted the 95% confidence interval of the estimate, hence the uncorrected estimate). Again, the effect of collectivism on a few emotions was notable in the uncorrected results. However, none of the effects survived the correction for multiple-comparison.

```{r echo = F, warning=F, message=F}
coef_table_gen <- data.frame(matrix(ncol = 3, nrow = 0))

for (i in 1:length(models_gen)){
  m <- models_gen[[i]]
  conf_int <- confint(m, "Collectivism")
  row <- c(m$coefficients[2], conf_int[1], conf_int[2])
  coef_table_gen <- rbind(coef_table_gen, row)
}
names(coef_table_gen) = c("Coefficient", "lower", "upper")

coef_table_gen$emotion <- varlist

ggplot(data = coef_table_gen) +
  geom_linerange(aes(xmin = lower, xmax = upper, y = emotion, color = emotion)) + 
  geom_point(aes(x = Coefficient, y = emotion)) + 
  geom_vline(aes(xintercept = 0)) +
  labs(x = "Effect of Collectivism", y = "Emotion",
       title = "Fig. 4. Effect of collectivism on different emotions of general tweets") +
  theme(legend.position = "none")
```


# Discussion

This study investigated how sentiment may change as a function of collectivism  in the face of pathogen threat by analyzing sentiment in geo-mapped tweets within the U.S. during COVID-19 pandemic. We started with a pair of competing hypotheses regarding the role of collectivism in modulating people's emotion under pathogen threat, and our results suggested that collectivism may serve as an emotional analgesic.

According to our results from vader analyses, collectivism predicted negative sentiment at the baseline level, yet it functioned as a buffer and predicted more positive sentiment in Covid-related tweets. This is consistent with the social psychological perspective where the sense of social embeddedness and mutual interdependence can reduce people's distress in the face of various threats [@coan2006lending; @Eisenberger2007; @Salvador2020]. Our NRC results showing that collectivism predicted the decrease of fear in covid-related tweet further supported this hypothesis.  

The rest of results from NRC analyses was less telling. In covid-related tweets, it was hard to interpret why collectivism predicted less trust  while predicting more positive sentiment at the same time. Moreover, collectivism did not predict any other specific positive sentiment. Similarly, in general tweets, collectivism failed to predict any specific negative sentiment. Putting these results together, it seems that one question awaiting for further investigations is which specific positive sentiment(s) drive people's emotional experiences in the time of pandemic.  

The present study took the advantage of web-based large-scale data to capture the moment-to-moment public sentiment during COVID-19. This method has several advantages over traditional lab-based studies. First, Twitter simply provided a bigger potential subject pool than most survey-based subject recruiting platforms. Secondly, the elusive and inconstant nature of mood makes self report a challenging task for subjects and an inaccurate measure for researchers. However, people spontaneously share their emotional experience on Twitter, which provides a natural and real-time reflection of their mood. Further, the rich and voluminous data obtained from long-time tweet streaming allows researcher to have a more representative sample of people's emotional experience. Thirdly, while subjects in a lab-setting may be concerned of expressing inappropriate emotions or behaviors, people are feeling much more comfortable sharing their authentic feelings on social media such as Twitter.

Finally, a few limitations must be acknowledged. First, though Twitter gave us access to a bigger group of subjects, this group cannot be considered as a representative sample of the U.S. or the world population. Secondly, the data collection and processing were conducted in two software, R and Python. Though we tried our best to produce functionally matching codes, the small misalignment between R and Python might still have cast influence on the results of the study. Finally, NRC has been found to produce more accurate results for longer texts[@reagan2017sentiment]. However, most tweets in our sample were within 3 sentences. The limited length of tweets may have led the NRC analyses to be less accurate, which in turn led to less telling and intuitive results.

In spite of the above mentioned limitations, this study has shed new light on the various roles of collectivism in modulating sentiment under pathogen threat. Future studies may benefit from exploring collectivism beyond the Western culture. Within the U.S., Hawaii in the present study had clearly distinguished itself as a unique subculture from the mainstream American culture, and it is possible that collectivism functions in a different manner in Hawaiians. On a global scale, previous studies have found that social relations may not always bring the sense of warmth and comfort in East Asian cultures [@Park2014]. Therefore, collectivism may also play a different role in these cultures.

# References

<div id="refs"></div>

\pagebreak

# Supplementary materials

## Supplementart Python codes

### Covid-related tweet-streaming
Below is the full python codes for collecting covid-related tweets. This method enables using keywords in the query, while at the same time limiting the collection within the U.S. bound box. The collection was conducted on each day during the three-week period.

```{python, eval = FALSE, python.reticulate = FALSE}
import tweepy
import pandas as pd
import time
import os

class MyStreamListener(tweepy.StreamListener):
    
    def __init__(self, time_limit = 600):
        self.start_time = time.time()
        self.limit = time_limit
        super(MyStreamListener, self).__init__()
    
    def on_status(self, status):
        if (time.time() - self.start_time) < self.limit:
            if hasattr(status, "extended_tweet"):
                text = status.extended_tweet["full_text"]
            else:
                text = status.text
            keys = ['covid', 'corona', 'pandemic', 'epidemic', 'reopen', 'quarantine',
                     'social distance', 'cough', 'fever', 'mask', 'virus', 'infect',
                     'contagious', 'stayhome']
            for term in keys:
                if term in text.lower():
                    out = " found"
                    print(out)
                    all_tweets.append(status)
                    break
            return True
        else:
            return False
        
        
    def on_error(self, status_code):
        if status_code == 420:
            print('paused')
            return False
            
all_tweets = []
box = [-178.334698, 18.910360999999998, -66.949895, 49.384358]
myStream = tweepy.Stream(auth = api.auth, listener = MyStreamListener(36000), 
                         tweet_mode = 'extended')
myStream.filter(locations = box)
```

### Geo-mapping in Python

```{python, eval = FALSE, python.reticulate = FALSE}
def sort_data(dates):
    f_name = dates + '_covid.pkl'
    infile = open(f_name, "rb")
    curr_day = pickle.load(infile)
    infile.close()
    all_states = {}
    #coords is a panda dataframe containing the name and coordinates of each state
    for i in range(coords.shape[0]):
        place = coords.loc[i, "NAME"]
        all_states[place] = []
    for twt in curr_day:
        if hasattr(twt, "place") and hasattr(twt.place, "country_code") \
        and hasattr(twt.place, "bounding_box"):
            if (twt.place.country_code == "US") and (twt.lang == "en"):
                loc_box = twt.place.bounding_box.coordinates[0]
                loc = np.mean(loc_box, axis = 0)
                coordinates = (loc[1], loc[0])
                r = rg.search(coordinates)
                place = r[0]['admin1']
                all_states[place].append(twt)
    outname = 'bystate_' + f_name
    f = open(outname, "wb")
    pickle.dump(all_states, f)
    f.close()
    return all_states
    
dates = ['1030', '1031', '1102', '1103', '1105', '1106', '1107', '1108', 
          '1109', '1110', '1111', '1112', '1113', '1114', '1115', '1116',
          '1117', '1118', '1119', '1120']
for day in dates:
    all_states = sort_data(day)
    #further processing starts here
    #See code chunk under sentiment analysis
```

### Sentiment analysis in Python

```{python, eval = FALSE, python.reticulate = FALSE}
for day in dates:
    all_states = sort_data(day)
    #container for vader scores
    vader_state = {}
    for i in all_states.keys():
        vader_state[i] = {'pos':[], 'neg':[], 'neu':[], 'compound':[]}
    #container for NRC scores
    NRC_state = {}
    for i in all_states.keys():
        NRC_state[i] = {'fear': [], 'anger': [], 'anticipation': [], \
                       'trust': [], 'surprise': [], 'positive': [], \
                       'negative': [], 'sadness': [], 'disgust': [], 'joy': [], \
                       'length': [], 'sentence': []}
    #looping through the data                   
    for s in all_states.keys():
        curr_states = all_states[s]
        if len(curr_states) > 0:
            #fetch the text of each tweet
            for i in range(len(curr_states)):
                tweet = curr_states[i]
                if hasattr(tweet, "extended_tweet"):
                    text = tweet.extended_tweet["full_text"]
                else:
                    text = tweet.text
                #Vader analysis
                broken_sent = nltk.sent_tokenize(text)
                sent_scores = {'pos':[], 'neg':[], 'neu':[], 'compound':[]}
                for sentence in broken_sent:
                    vs = analyzer.polarity_scores(sentence)
                    for k in vs.keys():
                        sent_scores[k].append(vs[k])
                for k in sent_scores.keys():
                    text_score = sum(sent_scores[k])/len(sent_scores[k])
                    vader_state[s][k].append(text_score)
                #NRC analysis
                text_object = NRCLex(text)
                text_score = text_object.affect_count
                text_length = len(text_object.words)
                NRC_state[s]['length'].append(text_length)
                NRC_state[s]['sentence'].append(len(text_object.sentences))
                for k in text_score.keys():
                    NRC_state[s][k].append(text_score[k])
```


## Supplementary R codes

### Fixing broken stream

Due to Internet interuption, json files for tweet streaming on 11/02 (pm), 11/04 (pm), 11/09 (am), 11/17 (pm) and 11/18 (pm) included broken tweets. The following codes (adopted from [this](https://gist.github.com/JBGruber/dee4c44e7d38d537426f57ba1e4f84ab) post on github) were used to detect and remove broken tweets so that the json files could be parsed into RDS files.

```{r broken tweets, eval=FALSE}
library(rtweet)
library(dplyr)
library(stringi)
library(anytime)

recover_stream <- function(path, dir = NULL, verbose = TRUE) {
 
  # read file and split to tweets
  lines <- readChar(path, file.info(path)$size, useBytes = TRUE)
  tweets <- stringi::stri_split_fixed(lines, "\n{")[[1]]
  tweets[-1] <- paste0("{", tweets[-1])
  tweets <- tweets[!(tweets == "" | tweets == "{")]
 
  # remove misbehaving characters
  tweets <- gsub("\r", "", tweets, fixed = TRUE)
  tweets <- gsub("\n", "", tweets, fixed = TRUE)
 
  # write tweets to disk and try to read them in individually
  if (is.null(dir)) {
    dir <- paste0(tempdir(), "/tweets/")
    dir.create(dir, showWarnings = FALSE)
  }
 
  if (verbose) {
    pb <- progress::progress_bar$new(
      format = "Processing tweets [:bar] :percent, :eta remaining",
      total = length(tweets), clear = FALSE
    )
    pb$tick(0)
  }
 
  tweets_l <- lapply(tweets, function(t) {
    pb$tick()
    id <- unlist(stringi::stri_extract_first_regex(t, "(?<=id\":)\\d+(?=,)"))[1]
    f <- paste0(dir, id, ".json")
    writeLines(t, f, useBytes = TRUE)
    out <- tryCatch(rtweet::parse_stream(f),
                    error = function(e) {})
    if ("tbl_df" %in% class(out)) {
      return(out)
    } else {
      return(id)
    }
  })
 
  # test which ones failed
  test <- vapply(tweets_l, is.character, FUN.VALUE = logical(1L))
  bad_files <- unlist(tweets_l[test])
 
  # Let user decide what to do
  if (length(bad_files) > 0) {
    message("There were ", length(bad_files),
            " tweets with problems. Should they be copied to your working directory?")
    sel <- menu(c("no", "yes", "copy a list with status_ids"))
    if (sel == 2) {
      dir.create(paste0(getwd(), "/broken_tweets/"), showWarnings = FALSE)
      file.copy(
        from = paste0(dir, bad_files, ".json"),
        to = paste0(getwd(), "/broken_tweets/", bad_files, ".json")
      )
    } else if (sel == 3) {
      writeLines(bad_files, "broken_tweets.txt")
    }
  }
 
  # clean up
  unlink(dir, recursive = TRUE)
 
  # return good tweets
  return(dplyr::bind_rows(tweets_l[!test]))
}
```
